# This file is used to manually specify the semi-automatic conversion of
# 'nipype.interfaces.afni.model.Remlfit' from Nipype to Pydra.
#
# Please fill-in/edit the fields below where appropriate
#
# Docs
# ----
# Performs Generalized least squares time series fit with Restricted
#     Maximum Likelihood (REML) estimation of the temporal auto-correlation
#     structure.
# 
#     For complete details, see the `3dREMLfit Documentation.
#     <https://afni.nimh.nih.gov/pub/dist/doc/program_help/3dREMLfit.html>`_
# 
#     Examples
#     ========
# 
#     >>> from nipype.interfaces import afni
#     >>> remlfit = afni.Remlfit()
#     >>> remlfit.inputs.in_files = ['functional.nii', 'functional2.nii']
#     >>> remlfit.inputs.out_file = 'output.nii'
#     >>> remlfit.inputs.matrix = 'output.1D'
#     >>> remlfit.inputs.gltsym = [('SYM: +Lab1 -Lab2', 'TestSYM'), ('timeseries.txt', 'TestFile')]
#     >>> remlfit.cmdline
#     '3dREMLfit -gltsym "SYM: +Lab1 -Lab2" TestSYM -gltsym "timeseries.txt" TestFile -input "functional.nii functional2.nii" -matrix output.1D -Rbuck output.nii'
#     >>> res = remlfit.run()  # doctest: +SKIP
#     
task_name: Remlfit
nipype_name: Remlfit
nipype_module: nipype.interfaces.afni.model
inputs:
  omit:
  # list[str] - fields to omit from the Pydra interface
  rename:
  # dict[str, str] - fields to rename in the Pydra interface
  types:
  # dict[str, type] - override inferred types (use "mime-like" string for file-format types,
  # e.g. 'medimage/nifti-gz'). For most fields the type will be correctly inferred
  # from the nipype interface, but you may want to be more specific, particularly
  # for file types, where specifying the format also specifies the file that will be
  # passed to the field in the automatically generated unittests.
    in_files: medimage/nifti1+list-of
    # type=inputmultiobject|default=[]: Read time series dataset
    matrix: medimage-afni/oned
    # type=file|default=<undefined>: the design matrix file, which should have been output from Deconvolve via the 'x1D' option
    matim: generic/file
    # type=file|default=<undefined>: read a standard file as the matrix. You can use only Col as a name in GLTs with these nonstandard matrix input methods, since the other names come from the 'matrix' file. These mutually exclusive options are ignored if 'matrix' is used.
    mask: generic/file
    # type=file|default=<undefined>: filename of 3D mask dataset; only data time series from within the mask will be analyzed; results for voxels outside the mask will be set to zero.
    STATmask: generic/file
    # type=file|default=<undefined>: filename of 3D mask dataset to be used for the purpose of reporting truncation-to float issues AND for computing the FDR curves. The actual results ARE not masked with this option (only with 'mask' or 'automask' options).
    addbase: generic/file+list-of
    # type=inputmultiobject|default=[]: file(s) to add baseline model columns to the matrix with this option. Each column in the specified file(s) will be appended to the matrix. File(s) must have at least as many rows as the matrix does.
    slibase: generic/file+list-of
    # type=inputmultiobject|default=[]: similar to 'addbase' in concept, BUT each specified file must have an integer multiple of the number of slices in the input dataset(s); then, separate regression matrices are generated for each slice, with the first column of the file appended to the matrix for the first slice of the dataset, the second column of the file appended to the matrix for the first slice of the dataset, and so on. Intended to help model physiological noise in FMRI, or other effects you want to regress out that might change significantly in the inter-slice time intervals. This will slow the program down, and make it use a lot more memory (to hold all the matrix stuff).
    slibase_sm: generic/file+list-of
    # type=inputmultiobject|default=[]: similar to 'slibase', BUT each file much be in slice major order (i.e. all slice0 columns come first, then all slice1 columns, etc).
    dsort: generic/file
    # type=file|default=<undefined>: 4D dataset to be used as voxelwise baseline regressor
    out_file: Path
    # type=file: dataset for beta + statistics from the REML estimation (if generated
    # type=file|default=<undefined>: output dataset for beta + statistics from the REML estimation; also contains the results of any GLT analysis requested in the Deconvolve setup, similar to the 'bucket' output from Deconvolve. This dataset does NOT get the betas (or statistics) of those regressors marked as 'baseline' in the matrix file.
    var_file: Path
    # type=file: dataset for REML variance parameters (if generated)
    # type=file|default=<undefined>: output dataset for REML variance parameters
    rbeta_file: Path
    # type=file: output dataset for beta weights from the REML estimation (if generated
    # type=file|default=<undefined>: output dataset for beta weights from the REML estimation, similar to the 'cbucket' output from Deconvolve. This dataset will contain all the beta weights, for baseline and stimulus regressors alike, unless the '-nobout' option is given -- in that case, this dataset will only get the betas for the stimulus regressors.
    glt_file: Path
    # type=file: output dataset for beta + statistics from the REML estimation, but ONLY for the GLTs added on the REMLfit command line itself via 'gltsym' (if generated)
    # type=file|default=<undefined>: output dataset for beta + statistics from the REML estimation, but ONLY for the GLTs added on the REMLfit command line itself via 'gltsym'; GLTs from Deconvolve's command line will NOT be included.
    fitts_file: Path
    # type=file: output dataset for REML fitted model (if generated)
    # type=file|default=<undefined>: output dataset for REML fitted model
    errts_file: Path
    # type=file: output dataset for REML residuals = data - fitted model (if generated
    # type=file|default=<undefined>: output dataset for REML residuals = data - fitted model
    wherr_file: Path
    # type=file: dataset for REML residual, whitened using the estimated ARMA(1,1) correlation matrix of the noise (if generated)
    # type=file|default=<undefined>: dataset for REML residual, whitened using the estimated ARMA(1,1) correlation matrix of the noise
    ovar: Path
    # type=file: dataset for OLSQ st.dev. parameter (if generated)
    # type=file|default=<undefined>: dataset for OLSQ st.dev. parameter (kind of boring)
    obeta: Path
    # type=file: dataset for beta weights from the OLSQ estimation (if generated)
    # type=file|default=<undefined>: dataset for beta weights from the OLSQ estimation
    obuck: Path
    # type=file: dataset for beta + statistics from the OLSQ estimation (if generated)
    # type=file|default=<undefined>: dataset for beta + statistics from the OLSQ estimation
    oglt: Path
    # type=file: dataset for beta + statistics from 'gltsym' options (if generated
    # type=file|default=<undefined>: dataset for beta + statistics from 'gltsym' options
    ofitts: Path
    # type=file: dataset for OLSQ fitted model (if generated)
    # type=file|default=<undefined>: dataset for OLSQ fitted model
    oerrts: Path
    # type=file: dataset for OLSQ residuals = data - fitted model (if generated
    # type=file|default=<undefined>: dataset for OLSQ residuals (data - fitted model)
  metadata:
  # dict[str, dict[str, any]] - additional metadata to set on any of the input fields (e.g. out_file: position: 1)
outputs:
  omit:
  # list[str] - fields to omit from the Pydra interface
  rename:
  # dict[str, str] - fields to rename in the Pydra interface
  types:
  # dict[str, type] - override inferred types (use "mime-like" string for file-format types,
  # e.g. 'medimage/nifti-gz'). For most fields the type will be correctly inferred
  # from the nipype interface, but you may want to be more specific, particularly
  # for file types, where specifying the format also specifies the file that will be
  # passed to the field in the automatically generated unittests.
    out_file: medimage/nifti1
    # type=file: dataset for beta + statistics from the REML estimation (if generated
    # type=file|default=<undefined>: output dataset for beta + statistics from the REML estimation; also contains the results of any GLT analysis requested in the Deconvolve setup, similar to the 'bucket' output from Deconvolve. This dataset does NOT get the betas (or statistics) of those regressors marked as 'baseline' in the matrix file.
    var_file: generic/file
    # type=file: dataset for REML variance parameters (if generated)
    # type=file|default=<undefined>: output dataset for REML variance parameters
    rbeta_file: generic/file
    # type=file: output dataset for beta weights from the REML estimation (if generated
    # type=file|default=<undefined>: output dataset for beta weights from the REML estimation, similar to the 'cbucket' output from Deconvolve. This dataset will contain all the beta weights, for baseline and stimulus regressors alike, unless the '-nobout' option is given -- in that case, this dataset will only get the betas for the stimulus regressors.
    glt_file: generic/file
    # type=file: output dataset for beta + statistics from the REML estimation, but ONLY for the GLTs added on the REMLfit command line itself via 'gltsym' (if generated)
    # type=file|default=<undefined>: output dataset for beta + statistics from the REML estimation, but ONLY for the GLTs added on the REMLfit command line itself via 'gltsym'; GLTs from Deconvolve's command line will NOT be included.
    fitts_file: generic/file
    # type=file: output dataset for REML fitted model (if generated)
    # type=file|default=<undefined>: output dataset for REML fitted model
    errts_file: generic/file
    # type=file: output dataset for REML residuals = data - fitted model (if generated
    # type=file|default=<undefined>: output dataset for REML residuals = data - fitted model
    wherr_file: generic/file
    # type=file: dataset for REML residual, whitened using the estimated ARMA(1,1) correlation matrix of the noise (if generated)
    # type=file|default=<undefined>: dataset for REML residual, whitened using the estimated ARMA(1,1) correlation matrix of the noise
    ovar: generic/file
    # type=file: dataset for OLSQ st.dev. parameter (if generated)
    # type=file|default=<undefined>: dataset for OLSQ st.dev. parameter (kind of boring)
    obeta: generic/file
    # type=file: dataset for beta weights from the OLSQ estimation (if generated)
    # type=file|default=<undefined>: dataset for beta weights from the OLSQ estimation
    obuck: generic/file
    # type=file: dataset for beta + statistics from the OLSQ estimation (if generated)
    # type=file|default=<undefined>: dataset for beta + statistics from the OLSQ estimation
    oglt: generic/file
    # type=file: dataset for beta + statistics from 'gltsym' options (if generated
    # type=file|default=<undefined>: dataset for beta + statistics from 'gltsym' options
    ofitts: generic/file
    # type=file: dataset for OLSQ fitted model (if generated)
    # type=file|default=<undefined>: dataset for OLSQ fitted model
    oerrts: generic/file
    # type=file: dataset for OLSQ residuals = data - fitted model (if generated
    # type=file|default=<undefined>: dataset for OLSQ residuals (data - fitted model)
  callables:
  # dict[str, str] - names of methods/callable classes defined in the adjacent `*_callables.py`
  # to set to the `callable` attribute of output fields
  templates:
  # dict[str, str] - `output_file_template` values to be provided to output fields
  requirements:
  # dict[str, list[str]] - input fields that are required to be provided for the output field to be present
tests:
- inputs:
  # dict[str, str] - values to provide to inputs fields in the task initialisation
  # (if not specified, will try to choose a sensible value)
    in_files:
    # type=inputmultiobject|default=[]: Read time series dataset
    matrix:
    # type=file|default=<undefined>: the design matrix file, which should have been output from Deconvolve via the 'x1D' option
    polort:
    # type=int|default=0: if no 'matrix' option is given, AND no 'matim' option, create a matrix with Legendre polynomial regressorsup to the specified order. The default value is 0, whichproduces a matrix with a single column of all ones
    matim:
    # type=file|default=<undefined>: read a standard file as the matrix. You can use only Col as a name in GLTs with these nonstandard matrix input methods, since the other names come from the 'matrix' file. These mutually exclusive options are ignored if 'matrix' is used.
    mask:
    # type=file|default=<undefined>: filename of 3D mask dataset; only data time series from within the mask will be analyzed; results for voxels outside the mask will be set to zero.
    automask:
    # type=bool|default=False: build a mask automatically from input data (will be slow for long time series datasets)
    STATmask:
    # type=file|default=<undefined>: filename of 3D mask dataset to be used for the purpose of reporting truncation-to float issues AND for computing the FDR curves. The actual results ARE not masked with this option (only with 'mask' or 'automask' options).
    addbase:
    # type=inputmultiobject|default=[]: file(s) to add baseline model columns to the matrix with this option. Each column in the specified file(s) will be appended to the matrix. File(s) must have at least as many rows as the matrix does.
    slibase:
    # type=inputmultiobject|default=[]: similar to 'addbase' in concept, BUT each specified file must have an integer multiple of the number of slices in the input dataset(s); then, separate regression matrices are generated for each slice, with the first column of the file appended to the matrix for the first slice of the dataset, the second column of the file appended to the matrix for the first slice of the dataset, and so on. Intended to help model physiological noise in FMRI, or other effects you want to regress out that might change significantly in the inter-slice time intervals. This will slow the program down, and make it use a lot more memory (to hold all the matrix stuff).
    slibase_sm:
    # type=inputmultiobject|default=[]: similar to 'slibase', BUT each file much be in slice major order (i.e. all slice0 columns come first, then all slice1 columns, etc).
    usetemp:
    # type=bool|default=False: write intermediate stuff to disk, to economize on RAM. Using this option might be necessary to run with 'slibase' and with 'Grid' values above the default, since the program has to store a large number of matrices for such a problem: two for every slice and for every (a,b) pair in the ARMA parameter grid. Temporary files are written to the directory given in environment variable TMPDIR, or in /tmp, or in ./ (preference is in that order)
    nodmbase:
    # type=bool|default=False: by default, baseline columns added to the matrix via 'addbase' or 'slibase' or 'dsort' will each have their mean removed (as is done in Deconvolve); this option turns this centering off
    dsort:
    # type=file|default=<undefined>: 4D dataset to be used as voxelwise baseline regressor
    dsort_nods:
    # type=bool|default=False: if 'dsort' option is used, this command will output additional results files excluding the 'dsort' file
    fout:
    # type=bool|default=False: output F-statistic for each stimulus
    rout:
    # type=bool|default=False: output the R^2 statistic for each stimulus
    tout:
    # type=bool|default=False: output the T-statistic for each stimulus; if you use 'out_file' and do not give any of 'fout', 'tout',or 'rout', then the program assumes 'fout' is activated.
    nofdr:
    # type=bool|default=False: do NOT add FDR curve data to bucket datasets; FDR curves can take a long time if 'tout' is used
    nobout:
    # type=bool|default=False: do NOT add baseline (null hypothesis) regressor betas to the 'rbeta_file' and/or 'obeta_file' output datasets.
    gltsym:
    # type=list|default=[]: read a symbolic GLT from input file and associate it with a label. As in Deconvolve, you can also use the 'SYM:' method to provide the definition of the GLT directly as a string (e.g., with 'SYM: +Label1 -Label2'). Unlike Deconvolve, you MUST specify 'SYM: ' if providing the GLT directly as a string instead of from a file
    out_file:
    # type=file: dataset for beta + statistics from the REML estimation (if generated
    # type=file|default=<undefined>: output dataset for beta + statistics from the REML estimation; also contains the results of any GLT analysis requested in the Deconvolve setup, similar to the 'bucket' output from Deconvolve. This dataset does NOT get the betas (or statistics) of those regressors marked as 'baseline' in the matrix file.
    var_file:
    # type=file: dataset for REML variance parameters (if generated)
    # type=file|default=<undefined>: output dataset for REML variance parameters
    rbeta_file:
    # type=file: output dataset for beta weights from the REML estimation (if generated
    # type=file|default=<undefined>: output dataset for beta weights from the REML estimation, similar to the 'cbucket' output from Deconvolve. This dataset will contain all the beta weights, for baseline and stimulus regressors alike, unless the '-nobout' option is given -- in that case, this dataset will only get the betas for the stimulus regressors.
    glt_file:
    # type=file: output dataset for beta + statistics from the REML estimation, but ONLY for the GLTs added on the REMLfit command line itself via 'gltsym' (if generated)
    # type=file|default=<undefined>: output dataset for beta + statistics from the REML estimation, but ONLY for the GLTs added on the REMLfit command line itself via 'gltsym'; GLTs from Deconvolve's command line will NOT be included.
    fitts_file:
    # type=file: output dataset for REML fitted model (if generated)
    # type=file|default=<undefined>: output dataset for REML fitted model
    errts_file:
    # type=file: output dataset for REML residuals = data - fitted model (if generated
    # type=file|default=<undefined>: output dataset for REML residuals = data - fitted model
    wherr_file:
    # type=file: dataset for REML residual, whitened using the estimated ARMA(1,1) correlation matrix of the noise (if generated)
    # type=file|default=<undefined>: dataset for REML residual, whitened using the estimated ARMA(1,1) correlation matrix of the noise
    quiet:
    # type=bool|default=False: turn off most progress messages
    verb:
    # type=bool|default=False: turns on more progress messages, including memory usage progress reports at various stages
    goforit:
    # type=bool|default=False: With potential issues flagged in the design matrix, an attempt will nevertheless be made to fit the model
    ovar:
    # type=file: dataset for OLSQ st.dev. parameter (if generated)
    # type=file|default=<undefined>: dataset for OLSQ st.dev. parameter (kind of boring)
    obeta:
    # type=file: dataset for beta weights from the OLSQ estimation (if generated)
    # type=file|default=<undefined>: dataset for beta weights from the OLSQ estimation
    obuck:
    # type=file: dataset for beta + statistics from the OLSQ estimation (if generated)
    # type=file|default=<undefined>: dataset for beta + statistics from the OLSQ estimation
    oglt:
    # type=file: dataset for beta + statistics from 'gltsym' options (if generated
    # type=file|default=<undefined>: dataset for beta + statistics from 'gltsym' options
    ofitts:
    # type=file: dataset for OLSQ fitted model (if generated)
    # type=file|default=<undefined>: dataset for OLSQ fitted model
    oerrts:
    # type=file: dataset for OLSQ residuals = data - fitted model (if generated
    # type=file|default=<undefined>: dataset for OLSQ residuals (data - fitted model)
    num_threads:
    # type=int|default=1: set number of threads
    outputtype:
    # type=enum|default='AFNI'|allowed['AFNI','NIFTI','NIFTI_GZ']: AFNI output filetype
    args:
    # type=str|default='': Additional parameters to the command
    environ:
    # type=dict|default={}: Environment variables
  imports:
  # list[nipype2pydra.task.base.importstatement] - list import statements required by the test, with each list item
  # consisting of 'module', 'name', and optionally 'alias' keys
  expected_outputs:
  # dict[str, str] - expected values for selected outputs, noting that tests will typically
  # be terminated before they complete for time-saving reasons, and therefore
  # these values will be ignored, when running in CI
  timeout: 10
  # int - the value to set for the timeout in the generated test, 
  # after which the test will be considered to have been initialised 
  # successfully. Set to 0 to disable the timeout (warning, this could
  # lead to the unittests taking a very long time to complete)
  xfail: true
  # bool - whether the unittest is expected to fail or not. Set to false
  # when you are satisfied with the edits you have made to this file
- inputs:
  # dict[str, str] - values to provide to inputs fields in the task initialisation
  # (if not specified, will try to choose a sensible value)
    in_files:
    # type=inputmultiobject|default=[]: Read time series dataset
    out_file: '"output.nii"'
    # type=file: dataset for beta + statistics from the REML estimation (if generated
    # type=file|default=<undefined>: output dataset for beta + statistics from the REML estimation; also contains the results of any GLT analysis requested in the Deconvolve setup, similar to the 'bucket' output from Deconvolve. This dataset does NOT get the betas (or statistics) of those regressors marked as 'baseline' in the matrix file.
    matrix:
    # type=file|default=<undefined>: the design matrix file, which should have been output from Deconvolve via the 'x1D' option
    gltsym: '[("SYM: +Lab1 -Lab2", "TestSYM"), ("timeseries.txt", "TestFile")]'
    # type=list|default=[]: read a symbolic GLT from input file and associate it with a label. As in Deconvolve, you can also use the 'SYM:' method to provide the definition of the GLT directly as a string (e.g., with 'SYM: +Label1 -Label2'). Unlike Deconvolve, you MUST specify 'SYM: ' if providing the GLT directly as a string instead of from a file
  imports:
  # list[nipype2pydra.task.base.importstatement] - list import statements required by the test, with each list item
  # consisting of 'module', 'name', and optionally 'alias' keys
  expected_outputs:
  # dict[str, str] - expected values for selected outputs, noting that tests will typically
  # be terminated before they complete for time-saving reasons, and therefore
  # these values will be ignored, when running in CI
  timeout: 10
  # int - the value to set for the timeout in the generated test, 
  # after which the test will be considered to have been initialised 
  # successfully. Set to 0 to disable the timeout (warning, this could
  # lead to the unittests taking a very long time to complete)
  xfail: true
  # bool - whether the unittest is expected to fail or not. Set to false
  # when you are satisfied with the edits you have made to this file
doctests:
- cmdline: '3dREMLfit -gltsym "SYM: +Lab1 -Lab2" TestSYM -gltsym "timeseries.txt" TestFile -input "functional.nii functional2.nii" -matrix output.1D -Rbuck output.nii'
  # str - the expected cmdline output
  inputs:
  # dict[str, str] - name-value pairs for inputs to be provided to the doctest.
  # If the field is of file-format type and the value is None, then the
  # '.mock()' method of the corresponding class is used instead.
    in_files:
    # type=inputmultiobject|default=[]: Read time series dataset
    out_file: '"output.nii"'
    # type=file: dataset for beta + statistics from the REML estimation (if generated
    # type=file|default=<undefined>: output dataset for beta + statistics from the REML estimation; also contains the results of any GLT analysis requested in the Deconvolve setup, similar to the 'bucket' output from Deconvolve. This dataset does NOT get the betas (or statistics) of those regressors marked as 'baseline' in the matrix file.
    matrix:
    # type=file|default=<undefined>: the design matrix file, which should have been output from Deconvolve via the 'x1D' option
    gltsym: '[("SYM: +Lab1 -Lab2", "TestSYM"), ("timeseries.txt", "TestFile")]'
    # type=list|default=[]: read a symbolic GLT from input file and associate it with a label. As in Deconvolve, you can also use the 'SYM:' method to provide the definition of the GLT directly as a string (e.g., with 'SYM: +Label1 -Label2'). Unlike Deconvolve, you MUST specify 'SYM: ' if providing the GLT directly as a string instead of from a file
  imports:
  # list[nipype2pydra.task.base.importstatement] - list import statements required by the test, with each list item
  # consisting of 'module', 'name', and optionally 'alias' keys
  directive:
  # str - any doctest directive to place on the cmdline call, e.g. # doctest: +ELLIPSIS
