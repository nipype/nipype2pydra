# This file is used to manually specify the semi-automatic conversion of
# 'nipype.interfaces.afni.utils.Bucket' from Nipype to Pydra.
#
# Please fill-in/edit the fields below where appropriate
#
# Docs
# ----
# Concatenate sub-bricks from input datasets into one big
#     'bucket' dataset.
# 
#     .. danger::
# 
#         Using this program, it is possible to create a dataset that
#         has different basic datum types for different sub-bricks
#         (e.g., shorts for brick 0, floats for brick 1).
#         Do NOT do this!  Very few AFNI programs will work correctly
#         with such datasets!
# 
#     Examples
#     --------
#     >>> from nipype.interfaces import afni
#     >>> bucket = afni.Bucket()
#     >>> bucket.inputs.in_file = [('functional.nii',"{2..$}"), ('functional.nii',"{1}")]
#     >>> bucket.inputs.out_file = 'vr_base'
#     >>> bucket.cmdline
#     "3dbucket -prefix vr_base functional.nii'{2..$}' functional.nii'{1}'"
#     >>> res = bucket.run()  # doctest: +SKIP
# 
#     See Also
#     --------
#     For complete details, see the `3dbucket Documentation.
#     <https://afni.nimh.nih.gov/pub/dist/doc/program_help/3dbucket.html>`__.
# 
#     
task_name: Bucket
nipype_name: Bucket
nipype_module: nipype.interfaces.afni.utils
inputs:
  omit:
  # list[str] - fields to omit from the Pydra interface
  rename:
  # dict[str, str] - fields to rename in the Pydra interface
  types:
  # dict[str, type] - override inferred types (use "mime-like" string for file-format types,
  # e.g. 'medimage/nifti-gz'). For most fields the type will be correctly inferred
  # from the nipype interface, but you may want to be more specific, particularly
  # for file types, where specifying the format also specifies the file that will be
  # passed to the field in the automatically generated unittests.
    out_file: Path
    # type=file: output file
    # type=file|default=<undefined>: 
  metadata:
  # dict[str, dict[str, any]] - additional metadata to set on any of the input fields (e.g. out_file: position: 1)
outputs:
  omit:
  # list[str] - fields to omit from the Pydra interface
  rename:
  # dict[str, str] - fields to rename in the Pydra interface
  types:
  # dict[str, type] - override inferred types (use "mime-like" string for file-format types,
  # e.g. 'medimage/nifti-gz'). For most fields the type will be correctly inferred
  # from the nipype interface, but you may want to be more specific, particularly
  # for file types, where specifying the format also specifies the file that will be
  # passed to the field in the automatically generated unittests.
    out_file: generic/file
    # type=file: output file
    # type=file|default=<undefined>: 
  callables:
  # dict[str, str] - names of methods/callable classes defined in the adjacent `*_callables.py`
  # to set to the `callable` attribute of output fields
  templates:
  # dict[str, str] - `output_file_template` values to be provided to output fields
  requirements:
  # dict[str, list[str]] - input fields that are required to be provided for the output field to be present
tests:
- inputs:
  # dict[str, str] - values to provide to inputs fields in the task initialisation
  # (if not specified, will try to choose a sensible value)
    in_file:
    # type=list|default=[]: List of tuples of input datasets and subbrick selection strings as described in more detail in the following afni help string Input dataset specified using one of these forms: ``prefix+view``, ``prefix+view.HEAD``, or ``prefix+view.BRIK``. You can also add a sub-brick selection list after the end of the dataset name.  This allows only a subset of the sub-bricks to be included into the output (by default, all of the input dataset is copied into the output).  A sub-brick selection list looks like one of the following forms::      fred+orig[5]                     ==> use only sub-brick #5     fred+orig[5,9,17]                ==> use #5, #9, and #17     fred+orig[5..8]     or [5-8]     ==> use #5, #6, #7, and #8     fred+orig[5..13(2)] or [5-13(2)] ==> use #5, #7, #9, #11, and #13  Sub-brick indexes start at 0.  You can use the character '$' to indicate the last sub-brick in a dataset; for example, you can select every third sub-brick by using the selection list ``fred+orig[0..$(3)]`` N.B.: The sub-bricks are output in the order specified, which may not be the order in the original datasets.  For example, using ``fred+orig[0..$(2),1..$(2)]`` will cause the sub-bricks in fred+orig to be output into the new dataset in an interleaved fashion. Using ``fred+orig[$..0]`` will reverse the order of the sub-bricks in the output. N.B.: Bucket datasets have multiple sub-bricks, but do NOT have a time dimension.  You can input sub-bricks from a 3D+time dataset into a bucket dataset.  You can use the '3dinfo' program to see how many sub-bricks a 3D+time or a bucket dataset contains. N.B.: In non-bucket functional datasets (like the 'fico' datasets output by FIM, or the 'fitt' datasets output by 3dttest), sub-brick ``[0]`` is the 'intensity' and sub-brick [1] is the statistical parameter used as a threshold.  Thus, to create a bucket dataset using the intensity from dataset A and the threshold from dataset B, and calling the output dataset C, you would type::      3dbucket -prefix C -fbuc 'A+orig[0]' -fbuc 'B+orig[1]  
    out_file:
    # type=file: output file
    # type=file|default=<undefined>: 
    num_threads:
    # type=int|default=1: set number of threads
    outputtype:
    # type=enum|default='AFNI'|allowed['AFNI','NIFTI','NIFTI_GZ']: AFNI output filetype
    args:
    # type=str|default='': Additional parameters to the command
    environ:
    # type=dict|default={}: Environment variables
  imports:
  # list[nipype2pydra.task.base.importstatement] - list import statements required by the test, with each list item
  # consisting of 'module', 'name', and optionally 'alias' keys
  expected_outputs:
  # dict[str, str] - expected values for selected outputs, noting that tests will typically
  # be terminated before they complete for time-saving reasons, and therefore
  # these values will be ignored, when running in CI
  timeout: 10
  # int - the value to set for the timeout in the generated test, 
  # after which the test will be considered to have been initialised 
  # successfully. Set to 0 to disable the timeout (warning, this could
  # lead to the unittests taking a very long time to complete)
  xfail: true
  # bool - whether the unittest is expected to fail or not. Set to false
  # when you are satisfied with the edits you have made to this file
- inputs:
  # dict[str, str] - values to provide to inputs fields in the task initialisation
  # (if not specified, will try to choose a sensible value)
    in_file: '[(''functional.nii'',"{2..$}"), (''functional.nii'',"{1}")]'
    # type=list|default=[]: List of tuples of input datasets and subbrick selection strings as described in more detail in the following afni help string Input dataset specified using one of these forms: ``prefix+view``, ``prefix+view.HEAD``, or ``prefix+view.BRIK``. You can also add a sub-brick selection list after the end of the dataset name.  This allows only a subset of the sub-bricks to be included into the output (by default, all of the input dataset is copied into the output).  A sub-brick selection list looks like one of the following forms::      fred+orig[5]                     ==> use only sub-brick #5     fred+orig[5,9,17]                ==> use #5, #9, and #17     fred+orig[5..8]     or [5-8]     ==> use #5, #6, #7, and #8     fred+orig[5..13(2)] or [5-13(2)] ==> use #5, #7, #9, #11, and #13  Sub-brick indexes start at 0.  You can use the character '$' to indicate the last sub-brick in a dataset; for example, you can select every third sub-brick by using the selection list ``fred+orig[0..$(3)]`` N.B.: The sub-bricks are output in the order specified, which may not be the order in the original datasets.  For example, using ``fred+orig[0..$(2),1..$(2)]`` will cause the sub-bricks in fred+orig to be output into the new dataset in an interleaved fashion. Using ``fred+orig[$..0]`` will reverse the order of the sub-bricks in the output. N.B.: Bucket datasets have multiple sub-bricks, but do NOT have a time dimension.  You can input sub-bricks from a 3D+time dataset into a bucket dataset.  You can use the '3dinfo' program to see how many sub-bricks a 3D+time or a bucket dataset contains. N.B.: In non-bucket functional datasets (like the 'fico' datasets output by FIM, or the 'fitt' datasets output by 3dttest), sub-brick ``[0]`` is the 'intensity' and sub-brick [1] is the statistical parameter used as a threshold.  Thus, to create a bucket dataset using the intensity from dataset A and the threshold from dataset B, and calling the output dataset C, you would type::      3dbucket -prefix C -fbuc 'A+orig[0]' -fbuc 'B+orig[1]  
    out_file: '"vr_base"'
    # type=file: output file
    # type=file|default=<undefined>: 
  imports:
  # list[nipype2pydra.task.base.importstatement] - list import statements required by the test, with each list item
  # consisting of 'module', 'name', and optionally 'alias' keys
  expected_outputs:
  # dict[str, str] - expected values for selected outputs, noting that tests will typically
  # be terminated before they complete for time-saving reasons, and therefore
  # these values will be ignored, when running in CI
  timeout: 10
  # int - the value to set for the timeout in the generated test, 
  # after which the test will be considered to have been initialised 
  # successfully. Set to 0 to disable the timeout (warning, this could
  # lead to the unittests taking a very long time to complete)
  xfail: true
  # bool - whether the unittest is expected to fail or not. Set to false
  # when you are satisfied with the edits you have made to this file
doctests:
- cmdline: 3dbucket -prefix vr_base functional.nii"{2..$}" functional.nii"{1}"
  # str - the expected cmdline output
  inputs:
  # dict[str, str] - name-value pairs for inputs to be provided to the doctest.
  # If the field is of file-format type and the value is None, then the
  # '.mock()' method of the corresponding class is used instead.
    in_file: '[(''functional.nii'',"{2..$}"), (''functional.nii'',"{1}")]'
    # type=list|default=[]: List of tuples of input datasets and subbrick selection strings as described in more detail in the following afni help string Input dataset specified using one of these forms: ``prefix+view``, ``prefix+view.HEAD``, or ``prefix+view.BRIK``. You can also add a sub-brick selection list after the end of the dataset name.  This allows only a subset of the sub-bricks to be included into the output (by default, all of the input dataset is copied into the output).  A sub-brick selection list looks like one of the following forms::      fred+orig[5]                     ==> use only sub-brick #5     fred+orig[5,9,17]                ==> use #5, #9, and #17     fred+orig[5..8]     or [5-8]     ==> use #5, #6, #7, and #8     fred+orig[5..13(2)] or [5-13(2)] ==> use #5, #7, #9, #11, and #13  Sub-brick indexes start at 0.  You can use the character '$' to indicate the last sub-brick in a dataset; for example, you can select every third sub-brick by using the selection list ``fred+orig[0..$(3)]`` N.B.: The sub-bricks are output in the order specified, which may not be the order in the original datasets.  For example, using ``fred+orig[0..$(2),1..$(2)]`` will cause the sub-bricks in fred+orig to be output into the new dataset in an interleaved fashion. Using ``fred+orig[$..0]`` will reverse the order of the sub-bricks in the output. N.B.: Bucket datasets have multiple sub-bricks, but do NOT have a time dimension.  You can input sub-bricks from a 3D+time dataset into a bucket dataset.  You can use the '3dinfo' program to see how many sub-bricks a 3D+time or a bucket dataset contains. N.B.: In non-bucket functional datasets (like the 'fico' datasets output by FIM, or the 'fitt' datasets output by 3dttest), sub-brick ``[0]`` is the 'intensity' and sub-brick [1] is the statistical parameter used as a threshold.  Thus, to create a bucket dataset using the intensity from dataset A and the threshold from dataset B, and calling the output dataset C, you would type::      3dbucket -prefix C -fbuc 'A+orig[0]' -fbuc 'B+orig[1]  
    out_file: '"vr_base"'
    # type=file: output file
    # type=file|default=<undefined>: 
  imports:
  # list[nipype2pydra.task.base.importstatement] - list import statements required by the test, with each list item
  # consisting of 'module', 'name', and optionally 'alias' keys
  directive:
  # str - any doctest directive to place on the cmdline call, e.g. # doctest: +ELLIPSIS
